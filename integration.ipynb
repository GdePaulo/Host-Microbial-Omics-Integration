{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import loader as load\n",
    "import processor as pr\n",
    "import config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.decomposition import NMF\n",
    "import json\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "# torch.cuda.manual_seed(1)\n",
    "# np.random.seed(1)\n",
    "\n",
    "NR_EXTRACTED_FEATURES = 30\n",
    "NR_OUTER_HIDDEN_LAYERS = 100\n",
    "CANCER = \"STAD\"\n",
    "# LAYER = \"aak_ge\"\n",
    "# LAYER = \"tcma_gen\"\n",
    "LAYER = \"tcma_gen_aak_ge\"\n",
    "NR_INPUT_FEATURES = len(config.modality_features[LAYER])\n",
    "\n",
    "# CANCER = \"COAD\"\n",
    "# CANCER = \"HNSC\"\n",
    "# CANCER = \"ESCA\"\n",
    "# CANCER = \"READ\"\n",
    "CANCER_MODEL_PATH = f\"{config.model_state_path}/{LAYER}/model_{CANCER.lower()}\"\n",
    "MODEL_PARAMETERS_PATH = f\"{config.model_state_path}/{LAYER}/best_parameters_{CANCER.lower()}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n",
    "\n",
    "class OverlapDataset(Dataset):\n",
    "    \"\"\"Genus + GE dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, layer, target, cancer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        data, files_names = load.loadAll(includeStage=(target==\"stage\"), sameSamples=True, skipGenes=(layer!=\"aak_ge\"), skipAE=True)\n",
    "        layer_data = load.getSpecificData(layer, data, files_names)\n",
    "\n",
    "        if target==\"tumor\":\n",
    "            layer_data = load.attachTumorStatus(layer_data)\n",
    "        else:\n",
    "            layer_data = load.attachStageStatus(layer_data)\n",
    "\n",
    "        if cancer != \"all\":\n",
    "            x, y = pr.splitData(layer_data, target=target, project=cancer)\n",
    "        else:\n",
    "            layer_data = layer_data.drop([\"project\"], axis=1)\n",
    "            x, y = pr.splitData(layer_data, target=target)\n",
    "\n",
    "        # x = x.drop(x.iloc[:, 20:5201], axis=1)\n",
    "        \n",
    "        self.modality_features = x\n",
    "        self.targets = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.modality_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample_features = self.modality_features.iloc[idx].values\n",
    "        sample_target = self.targets.iloc[idx]\n",
    "        sample = {'features': sample_features, 'target': sample_target}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped = OverlapDataset(LAYER, \"tumor\", CANCER)\n",
    "# Display text and label.\n",
    "print('\\nFirst iteration of data set: ', next(iter(overlapped)), '\\n')\n",
    "# Print how many items are in the data set\n",
    "print('Length of data set: ', len(overlapped), '\\n')\n",
    "# Print entire data set\n",
    "print('Entire data set: ', list(DataLoader(overlapped))[:2], '\\n')\n",
    "\n",
    "# DataLoader is used to load the dataset\n",
    "# for training\n",
    "loader = torch.utils.data.DataLoader(dataset = overlapped,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size = 16,\n",
    "\t\t\t\t\t\t\t\t\tshuffle = True)\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "print('Batched data set: ', list(loader)[:2], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://aacrjournals.org/clincancerres/article-pdf/24/6/1248/2049917/1248.pdf\n",
    "# Chaudhary paper: architecture = 500 -> 100 -> 500\n",
    "# Proportionately should be: 100 -> 30 -> 100\n",
    "# Features: 35877 -> 5221\n",
    "# Use further feature selection?\n",
    "# Use more complex training and testing pipeline with data splitting?\n",
    "class AE(torch.nn.Module):\n",
    "\tdef __init__(self, input_features=5221, first_hidden_size=NR_OUTER_HIDDEN_LAYERS, second_hidden_size=0, hidden_features=30):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tencoder_layers = []\n",
    "\t\tencoder_layers.append(torch.nn.Linear(input_features, first_hidden_size))\n",
    "\t\tencoder_layers.append(torch.nn.ReLU())\n",
    "\t\tif second_hidden_size > 0:\n",
    "\t\t\tencoder_layers.append(torch.nn.Linear(first_hidden_size, second_hidden_size))\n",
    "\t\t\tencoder_layers.append(torch.nn.ReLU())\n",
    "\t\t\tencoder_layers.append(torch.nn.Linear(second_hidden_size, hidden_features))\n",
    "\t\telse:\n",
    "\t\t\tencoder_layers.append(torch.nn.Linear(first_hidden_size, hidden_features))\n",
    "\t\t# Otherwise try leaky ReLu And otherwise give up\n",
    "\t\t# encoder_layers.append(torch.nn.ReLU())\n",
    "\t\tself.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\t\t\n",
    "\t\t# Building an linear decoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# The Sigmoid activation function\n",
    "\t\t# outputs the value between 0 and 1\n",
    "\t\t# 9 ==> 784\n",
    "\t\tdecoder_layers = []\n",
    "\n",
    "\t\tif second_hidden_size > 0:\n",
    "\t\t\tdecoder_layers.append(torch.nn.Linear(hidden_features, second_hidden_size))\n",
    "\t\t\tdecoder_layers.append(torch.nn.ReLU())\n",
    "\t\t\tdecoder_layers.append(torch.nn.Linear(second_hidden_size, first_hidden_size))\n",
    "\t\telse:\n",
    "\t\t\tdecoder_layers.append(torch.nn.Linear(hidden_features, first_hidden_size))\n",
    "\n",
    "\t\tdecoder_layers.append(torch.nn.ReLU())\n",
    "\n",
    "\t\t\n",
    "\t\tdecoder_layers.append(torch.nn.Linear(first_hidden_size, input_features))\n",
    "\t\tdecoder_layers.append(torch.nn.Sigmoid())\n",
    "\t\tself.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\t# HAS TO RETURN DECODED AND THEN ENCODED AS SK PIPELINE WILL TAKE FIRST VALUE WHEN COMPUTING LOSS IN GRIDSEARCH\n",
    "\t\treturn decoded, encoded\n",
    "\n",
    "class AutoEncoderNet(NeuralNetRegressor):\t\t\n",
    "\tdef get_loss(self, y_pred, y_true, *args, **kwargs):\n",
    "\t\tdecoded, encoded = y_pred\n",
    "\t\treturn super().get_loss(decoded, y_true, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_PARAMETERS_PATH) as f:\n",
    "    parameters = json.load(f)\n",
    "    \n",
    "print(parameters)\n",
    "best_epochs = parameters[\"general\"][\"max_epochs\"]\n",
    "# Model Initialization\n",
    "# model = AE(input_features=NR_INPUT_FEATURES, hidden_features=NR_EXTRACTED_FEATURES)\n",
    "model = AE(**parameters[\"module\"])\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), **parameters[\"optimizer\"])\n",
    "# optimizer = torch.optim.Adam(model.parameters(),\n",
    "# \t\t\t\t\t\t\tlr = 1e-2,\n",
    "# \t\t\t\t\t\t\tweight_decay = 1e-8)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped = OverlapDataset(LAYER, \"tumor\", CANCER)\n",
    "\n",
    "train_size = int(0.8 * len(overlapped))\n",
    "test_size = len(overlapped) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(overlapped, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size = 16,\n",
    "\t\t\t\t\t\t\t\t\tshuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size = 16,\n",
    "\t\t\t\t\t\t\t\t\tshuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"last hidden item: \", len(test_loader), len(train_dataset), len(test_dataset))\n",
    "for batch_id, batched_samples in enumerate(test_loader):\n",
    "    print(len(batched_samples[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = best_epochs\n",
    "outputs = []\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "hidden_representation = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss = 0.0\n",
    "\tvalid_loss = 0.0\n",
    "\n",
    "\tmodel.train(True)\n",
    "\tfor batch_id, batched_samples in enumerate(train_loader):\n",
    "\t\tfeatures = batched_samples[\"features\"].float()\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# Output of Autoencoder\n",
    "\t\treconstructed, hidden = model(features)\n",
    "\t\t\n",
    "\t\t# Calculating the loss function\n",
    "\t\tloss = loss_function(reconstructed, features)\n",
    "\t\t\n",
    "\t\t# The gradients are set to zero,\n",
    "\t\t# the gradient is computed and stored.\n",
    "\t\t# .step() performs parameter update\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\t# Storing the train_losses in a list for plotting\n",
    "\t\ttrain_loss += loss.detach() * len(batched_samples[\"features\"])\n",
    "\t\toutputs.append((epochs, features, reconstructed))\n",
    "\t\thidden_representation = hidden[-1]\n",
    "\n",
    "\t\tprint(\"last hidden item: \", hidden_representation)\n",
    "\t\tprint(\"last hidden item: \", len(batched_samples), len(train_loader))\n",
    "\t\n",
    "\n",
    "\tmodel.train(False)\n",
    "\tfor batch_id, batched_samples in enumerate(test_loader):\n",
    "\t\tfeatures = batched_samples[\"features\"].float()\n",
    "\t\treconstructed, hidden = model(features)\n",
    "\t\tloss = loss_function(reconstructed, features)\n",
    "\t\tvalid_loss += loss.detach() * len(batched_samples[\"features\"])\n",
    "\t\n",
    "\ttrain_losses.append(train_loss / len(train_dataset))\n",
    "\tvalidation_losses.append(valid_loss / len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plotting the last 100 values\n",
    "plt.plot(train_losses[:], label=\"train_loss\")\n",
    "plt.plot(validation_losses[:], label=\"validation_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), CANCER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-pytorch-deep-learning-models-with-scikit-learn/HYPE/\n",
    "# https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/Advanced_Usage.ipynb#scrollTo=GBsIqCiWsVCY\n",
    "\n",
    "# https://skorch.readthedocs.io/en/stable/user/neuralnet.html\n",
    "# https://github.com/skorch-dev/skorch/issues/451\n",
    "# R PARAMETER TUNING\n",
    "\n",
    "net = AutoEncoderNet(\n",
    "    AE,\n",
    "    max_epochs=10,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    # optimizer__lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    # Sets default split, not needed when using GridSearchCV\n",
    "    train_split=None,\n",
    "    # module__input_features=5221\n",
    "    # lr=0.0001,\n",
    "    # optimizer__weight_decay=1e-8,\n",
    "    # max_epochs=30,\n",
    "    # module__input_features=5221,\n",
    "    # module__first_hidden_size=300,\n",
    "    # module__second_hidden_size=0,\n",
    "    # module__hidden_features=100,\n",
    ")\n",
    "\n",
    "base_params = {\n",
    "    'optimizer__lr': [0.1, 0.01, 0.001, 0.0001][:],\n",
    "    'optimizer__weight_decay': [1e-8][:],\n",
    "    'max_epochs': [10, 20, 30, 40, 80][:],\n",
    "    # 'module__input_features': [5221][:],\n",
    "    'module__input_features': [NR_INPUT_FEATURES][:],\n",
    "}\n",
    "param_grid = [{\n",
    "    **base_params,\n",
    "    'module__first_hidden_size': [100, 200][:],\n",
    "    'module__second_hidden_size': [0][:],\n",
    "    'module__hidden_features': [50, 100][:],\n",
    "    }, {\n",
    "    **base_params,\n",
    "    'module__first_hidden_size': [100, 200][:],\n",
    "    'module__second_hidden_size': [0, 50][:],\n",
    "    'module__hidden_features': [30][:],\n",
    "    }, {\n",
    "    **base_params,\n",
    "    'module__first_hidden_size': [200][:],\n",
    "    'module__second_hidden_size': [100][:],\n",
    "    'module__hidden_features': [30, 50][:],\n",
    "    }, {\n",
    "    **base_params,\n",
    "    'module__first_hidden_size': [100, 200][:],\n",
    "    'module__second_hidden_size': [100][:],\n",
    "    'module__hidden_features': [30, 50, 100][:],\n",
    "    }, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test skorch wrapper model\n",
    "overlapped = OverlapDataset(LAYER, \"stage\", CANCER)\n",
    "X = torch.tensor(overlapped.modality_features.values, dtype=torch.float32)\n",
    "y = torch.tensor(overlapped.targets, dtype=torch.float32)\n",
    "net.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "# Implement custom stratified splitting, fix outputtiIng of score being nan (data related?), save model, save all scores for learning curve\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=net,\n",
    "    param_grid=param_grid,\n",
    "    cv=inner_cv,\n",
    "    scoring=\"neg_root_mean_squared_error\", #https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    refit=False,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    # Debug nan values\n",
    "    error_score=\"raise\")\n",
    "\n",
    "overlapped = OverlapDataset(LAYER, \"stage\", CANCER)\n",
    "X = torch.tensor(overlapped.modality_features.values, dtype=torch.float32)\n",
    "y = torch.tensor(overlapped.targets, dtype=torch.float32)\n",
    "result = grid_search.fit(X, X)\n",
    "# net.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.predict\n",
    "result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "print(means)\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_best_parameters = {\"module\":{}, \"optimizer\":{}, \"general\":{}}\n",
    "\n",
    "for k, v in result.best_params_.items():\n",
    "    if \"module__\" in k:\n",
    "        grouped_best_parameters[\"module\"][k[8:]] = v\n",
    "    if \"optimizer__\" in k:\n",
    "        grouped_best_parameters[\"optimizer\"][k[11:]] = v\n",
    "    else:\n",
    "        grouped_best_parameters[\"general\"][k] = v\n",
    "grouped_best_parameters\n",
    "\n",
    "load.createDirectory(MODEL_PARAMETERS_PATH)\n",
    "with open(MODEL_PARAMETERS_PATH, \"w+\") as f:\n",
    "    f.write(json.dumps(grouped_best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "import sklearn.metrics as metrics\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "fit_model = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, data, scorer=metrics.explained_variance_score):\n",
    "    \"\"\" Estimate performance of the model on the data \"\"\"\n",
    "    prediction = model.inverse_transform(model.transform(data))\n",
    "    return scorer(data, prediction)\n",
    "print(get_score(fit_model, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to feature normalized data? https://compgenomr.github.io/book/matrix-factorization-methods-for-unsupervised-multi-omics-data-integration.html#joint-non-negative-matrix-factorization \n",
    "# 19 Components has 67 separate construction error\n",
    "ks = [5, 20, 25, 30, 50]\n",
    "perfs_train = []\n",
    "perfs_test = []\n",
    "x = overlapped.modality_features.values\n",
    "for k in ks:\n",
    "    nmf = NMF(n_components=k).fit(x)\n",
    "    perfs_train.append(get_score(nmf, x))\n",
    "    perfs_test.append(get_score(nmf, x))\n",
    "    print(nmf.reconstruction_err_)\n",
    "print(perfs_train)\n",
    "print(perfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tst = nmf.transform(x)\n",
    "tst_or = overlapped.modality_features\n",
    "b = tst_or.iloc[:,:0]\n",
    "tst_df = pd.DataFrame(tst)\n",
    "pd.concat([b, tst_df], axis=1, ignore_index=True)\n",
    "# tst_df\n",
    "result = b.copy(deep=True)\n",
    "# for idx, row in b.iterrows():\n",
    "for col_nr in range(tst.shape[1]):\n",
    "    result[col_nr] = tst[:, col_nr] \n",
    "print(tst)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# layer_name = \"aak_ge\"\n",
    "# layer_name = \"tcma_gen\"\n",
    "layer_name = \"tcma_gen_aak_ge\"\n",
    "cancers = [\"STAD\"]\n",
    "with torch.no_grad():\n",
    "\n",
    "    def extractFeaturesAE(features, model):\n",
    "        features_tensor = torch.tensor(features.values).float()\n",
    "        _, hidden_representation = model(features_tensor)\n",
    "        return pd.Series(hidden_representation)\n",
    "\n",
    "    for target in config.prediction_targets[:]:\n",
    "        data, files_names = load.loadAll(includeStage=(target==\"stage\"), sameSamples=True, skipGenes=(layer_name!=\"aak_ge\"), skipAE=True)\n",
    "        layer_data = load.getSpecificData(layer_name, data, files_names)\n",
    "        \n",
    "        # print(ge_genus)\n",
    "\n",
    "        # do not attach the target status for stage or it will screw up the pipeline later on when it expects the raw target\n",
    "        if target == \"tumor\":\n",
    "            layer_data = load.attachTumorStatus(layer_data)\n",
    "\n",
    "        x_integrateds = []\n",
    "        \n",
    "        for extraction_method in [\"ae\", \"nmf\"][:1]:\n",
    "            # for c in [\"COAD\", \"ESCA\", \"HNSC\", \"READ\", \"STAD\"][:]:\n",
    "            for c in cancers[:]:\n",
    "                if c != \"all\":\n",
    "                    x, y = pr.splitData(layer_data, target=target, project=c)\n",
    "                else:\n",
    "                    layer_data = layer_data.drop([\"project\"], axis=1)\n",
    "                    x, y = pr.splitData(layer_data, target=target)\n",
    "\n",
    "                nr_features_extracted = 0\n",
    "                if extraction_method == \"ae\":\n",
    "                    nr_features_extracted = len(config.modality_features[layer_name + \"_ae\"])\n",
    "                    # nr_features_extracted = 30\n",
    "                    best_param_path = f\"{config.model_state_path}/{layer_name}/best_parameters_{c.lower()}.json\"\n",
    "                    with open(best_param_path) as f:\n",
    "                        parameters = json.load(f)\n",
    "\n",
    "                    model = AE(**parameters[\"module\"])\n",
    "                    current_cancer_model_path = f\"{config.model_state_path}/{layer_name}/model_{c.lower()}\"\n",
    "                    # print(current_cancer_model_path)\n",
    "                    model.load_state_dict(torch.load(current_cancer_model_path))\n",
    "                    model.eval()\n",
    "                \n",
    "                    x_integrated = x.apply(extractFeaturesAE, model=model, axis=1)\n",
    "                elif extraction_method == \"nmf\":\n",
    "                    nr_features_extracted = 30\n",
    "                    model = NMF(n_components=nr_features_extracted, init='random', random_state=0)\n",
    "                    nmf = model.fit(x.values)\n",
    "                    nmf_coef = nmf.transform(x.values)\n",
    "                    x_integrated = x.iloc[:, :0]\n",
    "                    for col_nr in range(nmf_coef.shape[1]):\n",
    "                        x_integrated[col_nr] = nmf_coef[:, col_nr]\n",
    "                x_integrated[target] = y\n",
    "                x_integrated[\"project\"] = c\n",
    "                    \n",
    "                print(c, x_integrated)\n",
    "                x_integrateds.append(x_integrated)\n",
    "            \n",
    "            print(x_integrateds)\n",
    "            integration_file_location = f\"Data/Integration/{extraction_method}/{target}/{layer_name}\"\n",
    "            load.createDirectory(integration_file_location)\n",
    "            ge_genus_integrated = pd.concat(x_integrateds)#, ignore_index=True)\n",
    "            \n",
    "            # Make sure final list has the same order as original list for to ensure training split happens the same every time\n",
    "            if len(cancers) > 1:\n",
    "                ge_genus_integrated = ge_genus_integrated.reindex(layer_data.index)\n",
    "            else:\n",
    "                # make the new index specific if there's only one cancer to avoid rows with missing values\n",
    "                cancer_layer_data = layer_data[layer_data[\"project\"]==cancers[0]]\n",
    "                ge_genus_integrated = ge_genus_integrated.reindex(cancer_layer_data.index)\n",
    "            # print(ge_genus_integrated)\n",
    "\n",
    "            ge_genus_integrated_minmaxed = ge_genus_integrated.copy()\n",
    "            latent_feature_names = ge_genus_integrated_minmaxed.columns[:nr_features_extracted]\n",
    "            ge_genus_integrated_minmaxed[latent_feature_names] = scaler.fit_transform(ge_genus_integrated_minmaxed[latent_feature_names])\n",
    "\n",
    "            ge_genus_integrated.to_csv(f\"{integration_file_location}.csv\")\n",
    "            ge_genus_integrated_minmaxed.to_csv(f\"{integration_file_location}_minmax.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.modality_features[layer_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bacteria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c9c23c68e4133fb152380ec4059b1f78568734598c8159b0962b671d1bd3454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
